diff --git a/rankod.py b/main.py
index 64046a4..6cc5897 100644
--- a/rankod.py
+++ b/main.py
@@ -5,37 +5,41 @@
 
 Source code for the REPEN algorithm in KDD'18. See the following paper for detail.
 Guansong Pang, Longbing Cao, Ling Chen, and Huan Liu. 2018. Learning Representations
-of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection. 
-In KDD 2018: 24th ACM SIGKDD International Conferenceon Knowledge Discovery & 
+of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection.
+In KDD 2018: 24th ACM SIGKDD International Conferenceon Knowledge Discovery &
 Data Mining, August 19â€“23, 2018, London, UnitedKingdom.
 
 This file is for experiments on csv data sets.
 """
 
-from IPython import get_ipython
-get_ipython().magic('reset -sf')
-
 import numpy as np
-np.random.seed(42)
+np.random.seed(123)
+
 import tensorflow as tf
-tf.set_random_seed(42)
+tf.set_random_seed(123)
+
 sess = tf.Session()
 
 from keras import backend as K
 K.set_session(sess)
 
+import sys
+
 from keras.layers import Input, Dense, Layer
 from keras.models import Model, load_model
-from keras.callbacks import ModelCheckpoint, TensorBoard
+from keras.callbacks import ModelCheckpoint, TensorBoard, Callback
 import matplotlib.pyplot as plt
 from sklearn.neighbors import NearestNeighbors, KDTree
 from sklearn.utils.random import sample_without_replacement
 import pandas as pd
 from sklearn.metrics.pairwise import pairwise_distances
 from utilities import dataLoading, cutoff_unsorted, aucPerformance, \
- normalization, writeRepresentation,writeResults, writeOutlierScores,visualizeData
+ normalization, writeRepresentation,writeResults, writeOutlierScores,visualizeData, \
+ prcPerformance
 import time
-
+from sklearn.model_selection import KFold, StratifiedKFold
+from sklearn.model_selection import TimeSeriesSplit
+from sklearn.metrics import precision_recall_curve
 MAX_INT = np.iinfo(np.int32).max
 MAX_FLOAT = np.finfo(np.float32).max
 #
@@ -46,8 +50,8 @@ MAX_FLOAT = np.finfo(np.float32).max
 #    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))
 
 def sqr_euclidean_dist(x,y):
-    return K.sum(K.square(x - y), axis=-1);
- 
+    return K.sum(K.square(x - y), axis=-1)
+
 
 class tripletRankingLossLayer(Layer):
     """Triplet ranking loss layer Class
@@ -56,17 +60,17 @@ class tripletRankingLossLayer(Layer):
     def __init__(self, **kwargs):
         self.is_placeholder = True
         super(tripletRankingLossLayer, self).__init__(**kwargs)
-        
+
 
 
     def rankingLoss(self, input_example, input_positive, input_negative):
         """Return the mean of the triplet ranking loss"""
-        
+
         positive_distances = sqr_euclidean_dist(input_example, input_positive)
         negative_distances = sqr_euclidean_dist(input_example, input_negative)
         loss = K.mean(K.maximum(0., 1000. - (negative_distances - positive_distances) ))
         return loss
-    
+
     def call(self, inputs):
         input_example = inputs[0]
         input_positive = inputs[1]
@@ -74,20 +78,20 @@ class tripletRankingLossLayer(Layer):
         loss = self.rankingLoss(input_example, input_positive, input_negative)
         self.add_loss(loss, inputs=inputs)
         # We won't actually use the output.
-        return input_example;
-     
- 
+        return input_example
+
+
 def lesinn(x_train):
     """the outlier scoring method, a bagging ensemble of Sp. See the following reference for detail.
-    Pang, Guansong, Kai Ming Ting, and David Albrecht. 
-    "LeSiNN: Detecting anomalies by identifying least similar nearest neighbours." 
+    Pang, Guansong, Kai Ming Ting, and David Albrecht.
+    "LeSiNN: Detecting anomalies by identifying least similar nearest neighbours."
     In Data Mining Workshop (ICDMW), 2015 IEEE International Conference on, pp. 623-630. IEEE, 2015.
     """
-    rng = np.random.RandomState(42) 
+    rng = np.random.RandomState(123)
     ensemble_size = 50
     subsample_size = 8
-    scores = np.zeros([x_train.shape[0], 1])  
-    # for reproductibility purpose  
+    scores = np.zeros([x_train.shape[0], 1])
+    # for reproductibility purpose
     seeds = rng.randint(MAX_INT, size = ensemble_size)
     for i in range(0, ensemble_size):
         rs = np.random.RandomState(seeds[i])
@@ -95,10 +99,10 @@ def lesinn(x_train):
         sid = sample_without_replacement(n_population = x_train.shape[0], n_samples = subsample_size, random_state = rs)
         subsample = x_train[sid]
         kdt = KDTree(subsample, metric='euclidean')
-        dists, indices = kdt.query(x_train, k = 1)       
+        dists, indices = kdt.query(x_train, k = 1)
         scores += dists
-    scores = scores / ensemble_size  
-    return scores;
+    scores = scores / ensemble_size
+    return scores
 
 
 def batch_generator(X, labels, batch_size, steps_per_epoch, scores, rng):
@@ -107,7 +111,7 @@ def batch_generator(X, labels, batch_size, steps_per_epoch, scores, rng):
     number_of_batches = steps_per_epoch
     rng = np.random.RandomState(rng.randint(MAX_INT, size = 1))
     counter = 0
-    while 1:        
+    while 1:
         X1, X2, X3 = tripletBatchGeneration(X, batch_size, rng, scores)
         counter += 1
         yield([np.array(X1), np.array(X2), np.array(X3)], None)
@@ -129,141 +133,257 @@ def tripletBatchGeneration(X, batch_size, rng, outlier_scores):
     examples = np.zeros([batch_size]).astype('int')
     positives = np.zeros([batch_size]).astype('int')
     negatives = np.zeros([batch_size]).astype('int')
-    
+
     for i in range(0, batch_size):
         sid = rng.choice(len(inlier_ids), 1, p = positive_weights)
         examples[i] = inlier_ids[sid]
-        
+
         sid2 = rng.choice(len(inlier_ids), 1)
-        
+
         while sid2 == sid:
-            sid2 = rng.choice(len(inlier_ids), 1)        
+            sid2 = rng.choice(len(inlier_ids), 1)
         positives[i] = inlier_ids[sid2]
         sid = rng.choice(len(outlier_ids), 1, p = negative_weights)
         negatives[i] = outlier_ids[sid]
     examples = X[examples, :]
     positives = X[positives, :]
     negatives = X[negatives, :]
-    return examples, positives, negatives;
+    return examples, positives, negatives
 
 
-    
-def tripletModel(input_dim, embedding_size = 20): 
+
+def tripletModel(input_dim, embedding_size = 20):
     """the learning model
     """
 
     input_e = Input(shape=(input_dim,), name = 'input_e')
     input_p = Input(shape=(input_dim,), name = 'input_p')
     input_n = Input(shape=(input_dim,), name = 'input_n')
-    
+
     hidden_layer = Dense(embedding_size, activation='relu', name = 'hidden_layer')
     hidden_e = hidden_layer(input_e)
     hidden_p = hidden_layer(input_p)
     hidden_n = hidden_layer(input_n)
-    
+
     output_layer = tripletRankingLossLayer()([hidden_e,hidden_p,hidden_n])
-    
+
     rankModel = Model(inputs=[input_e, input_p, input_n], outputs=output_layer)
-    
+
     representation = Model(inputs=input_e, outputs=hidden_e)
-    
+
     print(rankModel.summary(), representation.summary())
-    return rankModel, representation;
+    return rankModel, representation
 
-    
-def training_model(rankModel, X, labels,embedding_size, scores, filename, ite_num, rng = None):  
+
+def training_model(rankModel, X, labels, x_test, y_test, embedding_size, scores, filename, fold, rng=None):
     """training the model
     """
-    
-    rankModel.compile(optimizer = 'adadelta', loss = None)
-    
-    checkpointer = ModelCheckpoint("./model/" + str(embedding_size) + "D_" + str(ite_num) + "_"+ filename + ".h5", monitor='loss',
-                               verbose=0, save_best_only = True, save_weights_only=True)
-    
-    
     # training
-    batch_size = 256    
-#    samples_per_epoch = 5000
-    samples_per_epoch = X.shape[0]
+    batch_size = 256
+    samples_per_epoch = 5000
+    epochs = 100000000 # we want this to run for 12h,24h,...
+    #    samples_per_epoch = X.shape[0]
     steps_per_epoch = samples_per_epoch / batch_size
+    acc = 0
+
+    rankModel.compile(optimizer='adadelta', loss=None)
+    modelName = "./model/" + str(embedding_size) + "D_" + str(fold) + "_" + filename + '.h5'
+    checkpointer = ModelCheckpoint("./model/" + str(embedding_size) + "D_" + str(fold) + "_"+ filename + ".h5", monitor='loss',
+                               verbose=0, save_best_only=True, save_weights_only=True)
+
+    ### CODE ADDED FOR SURVEY ###
+    # instantiate custom callback object
+    epoch_recorder = CustomCallback(modelName=modelName, x_test=x_test, y_test=y_test, filename=filename, epochs=epochs, fold_num=fold)
+
+
+
     history = rankModel.fit_generator(batch_generator(X, labels, batch_size, steps_per_epoch, scores, rng),
                               steps_per_epoch = steps_per_epoch,
-                              epochs = 30,
+                              epochs = epochs,
                               shuffle = False,
-                              callbacks=[checkpointer])
-    plt.figure(figsize=(5, 5))
-    plt.plot(history.history['loss'])
-    plt.grid()
-    plt.title('model loss')
-    plt.xlabel('epochs')
-    plt.ylabel('loss')
-    plt.show()
-    
+                              callbacks=[checkpointer,epoch_recorder],
+                              verbose = 0)
+    # plt.figure(figsize=(5, 5))
+    # plt.plot(history.history['loss'])
+    # plt.grid()
+    # plt.title('model loss')
+    # plt.xlabel('epochs')
+    # plt.ylabel('loss')
+    # plt.show()
+
 
 def load_model_predict(model_name, X, labels, embedding_size, filename):
     """load the representation learning model and do the mappings.
     LeSiNN, the Sp ensemble, is applied to perform outlier scoring
     in the representation space.
     """
-    rankModel, representation = tripletModel(X.shape[1], embedding_size=20)  
+    rankModel, representation = tripletModel(X.shape[1], embedding_size=20)
+    rankModel.load_weights(model_name)
+    representation = Model(inputs=rankModel.input[0],
+                                 outputs=rankModel.get_layer('hidden_layer').get_output_at(0))
+
+    new_X = representation.predict(X)
+#    writeRepresentation(new_X, labels, embedding_size, filename + str(embedding_size) + "D_RankOD")
+    scores = lesinn(new_X)
+
+    rauc = aucPerformance(scores, labels)
+    pauc = prcPerformance(scores, labels)
+    writeResults(filename, embedding_size, rauc, pauc)
+#    writeOutlierScores(scores, labels, str(embedding_size) + "D_"+filename)
+    return rauc, pauc
+
+### CODE ADDED FOR SURVEY ###
+# Testing function for epoch level testing
+def load_epoch_predict(model_name, X, labels, embedding_size, filename):
+    """load the representation learning model and do the mappings.
+    LeSiNN, the Sp ensemble, is applied to perform outlier scoring
+    in the representation space.
+    """
+    rankModel, representation = tripletModel(X.shape[1], embedding_size=20)
     rankModel.load_weights(model_name)
     representation = Model(inputs=rankModel.input[0],
                                  outputs=rankModel.get_layer('hidden_layer').get_output_at(0))
-    
-    new_X = representation.predict(X)  
+
+    new_X = representation.predict(X)
 #    writeRepresentation(new_X, labels, embedding_size, filename + str(embedding_size) + "D_RankOD")
     scores = lesinn(new_X)
+
+    precision, recall, thresholds = precision_recall_curve(labels, scores)
+    numerator = 2 * recall * precision
+    denom = recall + precision
+    f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom != 0))
+    max_f1 = np.max(f1_scores)
+    #max_f1_thresh = thresholds[np.argmax(f1_scores)]
+
     rauc = aucPerformance(scores, labels)
-    writeResults(filename, embedding_size, rauc)
+    pauc = prcPerformance(scores, labels)
+    #writeResults(filename, embedding_size, rauc, pauc)
 #    writeOutlierScores(scores, labels, str(embedding_size) + "D_"+filename)
-    return rauc
+    return rauc, pauc, max_f1
 
-def test_diff_embeddings(X, labels, outlier_scores, filename):
+def test_diff_embeddings(X, labels, outlier_scores, filename, Y, y_labels):
     """sensitivity test w.r.t. different representation dimensions
     """
     embeddings = np.array([1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
     for j in range(0,len(embeddings)):
         embedding_size = embeddings[j]
-        test_single_embedding(X, labels, outlier_scores, filename, embedding_size)
-        
+        test_single_embedding(X, labels, outlier_scores, filename, Y, y_labels, embedding_size)
 
-def test_single_embedding(X, labels, outlier_scores, filename, embedding_size = 20):
+
+def test_single_embedding(X, labels, outlier_scores, filename, wfold, embedding_size=20):
     """perform representation learning with a fixed representation dimension
     and outlier detection using LeSiNN
     """
-    runs = 10
-    rauc = np.empty([runs, 1])    
-    rng = np.random.RandomState(42) 
-    for i in range(0,runs):
-        rankModel, representation = tripletModel(X.shape[1], embedding_size)    
-        training_model(rankModel, X, labels, embedding_size, outlier_scores, filename, i, rng)
-        
-        modelName = "./model/" + str(embedding_size) + "D_" + str(i)+ "_" + filename + '.h5'
-        rauc[i] = load_model_predict(modelName, X, labels, embedding_size,filename)
+
+    rng = np.random.RandomState(123)
+    n_split = 4
+    rauc = np.empty([n_split, 1])
+    prauc = np.empty([n_split, 1])
+    fold = 0
+    which_fold = wfold #[0,1,2,3]
+    for train_index, test_index in StratifiedKFold(n_split, random_state=123).split(X, labels):
+        if fold != which_fold:
+            print('ignore fold')
+        else:
+            # ======Added for Survey Paper====== #
+            x_train, x_test = X[train_index], X[test_index]
+            y_train, y_test = labels[train_index], labels[test_index]
+            train_scores, test_scores = outlier_scores[train_index], outlier_scores[test_index]
+            # ======Added for Survey Paper====== #
+
+            rankModel, representation = tripletModel(x_train.shape[1], embedding_size)
+
+            # ======Added for Survey Paper====== #
+            start_time = time.time()
+            # ======Added for Survey Paper====== #
+
+            training_model(rankModel, x_train, y_train, x_test, y_test, embedding_size, train_scores, filename, fold, rng)
+
+            ### CODE BELOW HERE IS NEVER REACHED IN THIS IMPLEMENTATION ###
+
+            # ======Added for Survey Paper====== #
+            print("Training Time:", time.time() - start_time)
+            # ======Added for Survey Paper====== #
+            modelName = "./model/" + str(embedding_size) + "D_" + str(fold)+ "_" + filename + '.h5'
+
+            # ======Added for Survey Paper====== #
+            test_time = time.time()
+            # ======Added for Survey Paper====== #
+            rauc[fold], prauc[fold] = load_model_predict(modelName, x_test, y_test, embedding_size,filename)
+
+            # ======Added for Survey Paper====== #
+            print("Testing Time:", time.time() - test_time)
+            # ======Added for Survey Paper====== #
+
+        fold += 1
     mean_auc = np.mean(rauc)
     s_auc = np.std(rauc)
+    mean_prauc = np.mean(prauc)
+    s_prauc = np.std(prauc)
 #    print(mean_auc)
-    writeResults(filename, embedding_size, mean_auc, std_auc = s_auc)
+    writeResults(filename, embedding_size, mean_auc, mean_prauc, std_auc=s_auc, std_prauc=s_prauc)
 
-## specify data files        
-filename = 'lung-1vs5'
-X, labels = dataLoading("./data/" + filename + ".csv")
+###CODE ADDED FOR SURVEY ###
+# custom callback class so that we can record results per epoch
+class CustomCallback(Callback):
+    # initialize the custom callback class with the necessary data
+    def __init__(self, modelName, x_test, y_test, filename, epochs, fold_num, embedding_size=20):
+        self.modelName = modelName
+        self.x_test = x_test
+        self.y_test = y_test
+        self.filename = filename
+        self.embedding_size = embedding_size
+        self.fold_num = fold_num
+        self.rauc = np.empty([0, 1])
+        self.pauc = np.empty([0, 1])
+        self.f1 = np.empty([0, 1])
+        self.test_time = np.empty([0, 1])
+        self.train_time = np.empty([0, 1])
+
+    # Code runs when an new epoch begins
+    def on_epoch_begin(self, epoch, logs=None):
+        # save the start time of the epoch/training session
+        self.train_time = np.append(self.train_time, time.time())
 
-#start_time = time.time() 
- 
-outlier_scores = lesinn(X) 
+    # Code runs when an epoch ends
+    def on_epoch_end(self, epoch, logs=None):
+        # Overwrite and save total epoch training time
+        self.train_time[epoch] = time.time() - self.train_time[epoch]
+
+        # record testing time and test model
+        start_time = time.time()
+        rauc, pauc, f1 = load_epoch_predict(self.modelName, self.x_test, self.y_test, self.embedding_size,
+                                            self.filename)
+        tt = time.time() - start_time
+
+        # append times and scores to their respective numpy arrays
+        self.test_time = np.append(self.test_time, tt)
+        self.rauc = np.append(self.rauc, rauc)
+        self.pauc = np.append(self.pauc, pauc)
+        self.f1 = np.append(self.f1, f1)
+
+        # save arrays to a single csv
+        df = pd.DataFrame(list(zip(self.rauc, self.pauc, self.f1, self.train_time, self.test_time)),
+                          columns=['ROC_AUC', 'PR_AUC', 'F1', 'Train Time', 'Test Time'])
+        df.to_csv("results/" + filename + "_fold_" + str(self.fold_num) + ".csv")
+
+
+
+try:
+    filename = str(sys.argv[1])
+    test_fold = int(sys.argv[2])
+except(IndexError):
+    raise IndexError('Dataset Name (without .csv) and Experiment Fold (0-3) must be supplied in the command line: \n python(3) main.py nslkdd_100 2')
+
+## specify data files
+X, labels = dataLoading("./data/" + filename + ".csv")
 
-##load the pre-saved outlier scores directly
-#df = pd.read_csv('./outlierscores/' + filename + ".csv") 
-#outlier_scores = df['score'].values
-#labels = df['class'].values
+start_time = time.time()
+outlier_scores = lesinn(X)
 
 rauc = aucPerformance(outlier_scores, labels)
-#writeResults(filename, X.shape[1], rauc)
-#outlier_scores = None
-test_single_embedding(X, labels, outlier_scores, filename)
-#print("--- %s seconds ---" % (time.time() - start_time))
-#writeOutlierScores(outlier_scores, labels, filename)
-#test_diff_embeddings(X, labels, outlier_scores, filename)
 
+test_single_embedding(X, labels, outlier_scores, filename, test_fold)
 
+### In this implementation, this code is never reached ###
+print("--- %s seconds ---" % (time.time() - start_time))
